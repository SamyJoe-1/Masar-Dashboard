import os
import re
import time
from typing import Optional, Dict, Any

from fastapi import FastAPI, Body, Request, Form
from fastapi.responses import JSONResponse, PlainTextResponse
from pydantic import BaseModel, Field

# -------- .env (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) --------
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# -------- OpenAI --------
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL   = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

OPENAI_OK = True
_client = None
try:
    from openai import OpenAI
    if OPENAI_API_KEY:
        _client = OpenAI(api_key=OPENAI_API_KEY)
    else:
        OPENAI_OK = False
except Exception:
    OPENAI_OK = False
    _client = None

app = FastAPI(
    title="Resume ATS UPDATE â€“ AI Job Title API",
    version="1.1.0",
    description="AI agent ÙŠÙÙ‡Ù… Job Description ÙˆÙŠÙØ±Ø¬Ø¹ Job Title Ù†Ø¸ÙŠÙ (Ø¨Ø¯ÙˆÙ† seniority/company). ÙŠÙ‚Ø¨Ù„ JSON Ø£Ùˆ text/plain."
)

# -------- Models --------
class JDRequest(BaseModel):
    job_description: str = Field(..., description="Ù†Øµ Ø§Ù„Ù€ JD ÙƒØ§Ù…Ù„ (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)")
    debug: Optional[bool] = Field(False, description="Ø£Ø¹Ø¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° ÙÙŠ Ø§Ù„Ø±Ø¯")

class JDResponse(BaseModel):
    job_title: str
    source: str                 # llm | fallback | failed
    model: Optional[str] = None
    elapsed_ms: int
    note: Optional[str] = None
    error: Optional[str] = None
    log: Optional[Dict[str, Any]] = None

# -------- Cleaning helpers --------
SENIORITY_WORDS = {
    # English
    "senior","jr","sr","junior","middle","mid-level","mid level",
    "principal","lead","head","intern","graduate","trainee","entry-level","entry level",
    # Arabic common
    "ÙƒØ¨ÙŠØ±","Ø³ÙŠÙ†ÙŠØ±","Ø³ÙŠÙ†ÙŠÙˆØ±","Ø¬ÙˆÙ†ÙŠÙˆØ±","Ù…Ø¨ØªØ¯Ø¦","Ù…ØªØ¯Ø±Ø¨","Ø±Ø¦ÙŠØ³","Ù‚Ø§Ø¦Ø¯"
}
ROLE_NORMALIZE_RE = re.compile(r"\s{2,}")

def strip_seniority_company(title: str) -> str:
    t = title.strip()
    if not t:
        return t
    # remove seniority words
    t = re.sub(r"(?i)\b(" + "|".join(map(re.escape, SENIORITY_WORDS)) + r")\b", " ", t)
    # remove at / with Company
    t = re.split(r"(?i)\b(at|with|ÙÙŠ|Ù„Ø¯Ù‰)\b", t)[0]
    # basic tidy
    t = t.replace("â€¢", " ").strip(" .,:;\"'â€œâ€â€˜â€™-/\\|()[]")
    t = ROLE_NORMALIZE_RE.sub(" ", t)
    return t[:80].strip()

# -------- Heuristic fallback (EN/AR) --------
PATTERNS = [
    r"(?i)job\s*title\s*:\s*([A-Za-z][A-Za-z0-9\s/&\-\(\)]{2,80})",
    r"(?i)position\s*:\s*([A-Za-z][A-Za-z0-9\s/&\-\(\)]{2,80})",
    r"(?i)role\s*:\s*([A-Za-z][A-Za-z0-9\s/&\-\(\)]{2,80})",
    r"(?i)as an?\s+([A-Za-z][A-Za-z0-9\s/&\-\(\)]{2,80})",
    r"\b([A-Za-z][A-Za-z\s/&\-]*(Engineer|Developer|Analyst|Scientist|Manager|Consultant|Specialist|Designer|Architect|Administrator|Coordinator|Officer|Product Manager|Project Manager|Program Manager))\b",
    # Arabic
    r"(?i)Ø§Ù„Ù…Ø³Ù…Ù‰\s*Ø§Ù„ÙˆØ¸ÙŠÙÙŠ\s*[:ï¼š]\s*([^\n\r]{2,80})",
    r"(?i)Ø§Ù„ÙˆØ¸ÙŠÙØ©\s*[:ï¼š]\s*([^\n\r]{2,80})",
    r"(?i)Ù†Ø¨Ø­Ø«\s+Ø¹Ù†\s+([^\n\r]{2,80})",
    r"(?i)Ùƒ\s*Ù€?\s*([^\n\r]{2,80})\s+Ø³ØªÙ‚ÙˆÙ…",  # "ÙƒÙ€ Ù…Ù‡Ù†Ø¯Ø³ ... Ø³ØªÙ‚ÙˆÙ…"
]

def heuristic_fallback(jd: str) -> str:
    text = jd.strip()
    if not text:
        return "-"
    for pat in PATTERNS:
        m = re.search(pat, text)
        if m:
            raw = m.group(1).strip()
            raw = re.split(r"[\n\r.ØŒ:;|/]", raw)[0]
            clean = strip_seniority_company(raw)
            if clean:
                return clean
    first = text.splitlines()[0] if "\n" in text else text
    first = re.split(r"[:\.\-â€“ØŒØ›]", first)[0]
    return strip_seniority_company(first) or "-"

# -------- LLM inference --------
SYS_PROMPT = (
    "You are a precise extractor. Given a job description (in English or Arabic), "
    "return ONLY the clean job title without seniority (no Senior/Junior/Lead/Head) and without company names. "
    "Keep it short (max 6 words). Respond with plain text only."
)

def llm_infer_title(jd: str) -> str:
    if not OPENAI_OK or not _client:
        raise RuntimeError("OpenAI client not configured")
    resp = _client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": SYS_PROMPT},
            {"role": "user", "content": jd},
        ],
        temperature=0,
        max_tokens=24,
    )
    content = (resp.choices[0].message.content or "").strip()
    content = content.replace("\n", " ").strip(" \"'â€œâ€â€˜â€™.ØŒ")
    return strip_seniority_company(content)

# -------- Common pipeline --------
def infer_pipeline(jd: str, debug: bool=False) -> JDResponse:
    t0 = time.time()
    logs = {"steps": [], "len": len(jd or "")} if debug else None
    try:
        if OPENAI_OK and _client and OPENAI_API_KEY:
            if logs: logs["steps"].append("llm")
            title = llm_infer_title(jd)
            return JDResponse(
                job_title=title or "-",
                source="llm",
                model=OPENAI_MODEL,
                elapsed_ms=int((time.time() - t0) * 1000),
                log=logs
            )
        else:
            raise RuntimeError("LLM disabled")
    except Exception as e:
        if logs: logs["steps"].append(f"llm_error: {e}")
        try:
            fb = heuristic_fallback(jd)
            return JDResponse(
                job_title=fb or "-",
                source="fallback",
                model=None,
                elapsed_ms=int((time.time() - t0) * 1000),
                note="Returned by local heuristic fallback.",
                error=str(e) if debug else None,
                log=logs
            )
        except Exception as e2:
            return JDResponse(
                job_title="-",
                source="failed",
                model=None,
                elapsed_ms=int((time.time() - t0) * 1000),
                error=f"LLM & Fallback failed: {e2}",
                log=logs
            )

# -------- Endpoints --------
@app.get("/")
def home():
    return {"message": "ğŸš€ AI Job Title API running", "docs": "/docs"}

# (A) Ø°ÙƒØ§Ø¡: Ù†ÙØ³ Ø§Ù„Ù…Ø³Ø§Ø± ÙŠÙ‚Ø¨Ù„ JSON Ø£Ùˆ Ù†Øµ Ø®Ø§Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
@app.post("/ai-job-title", response_model=JDResponse)
async def ai_job_title(request: Request):
    """
    ÙŠÙ‚Ø¨Ù„:
      - application/json: { "job_description": "...", "debug": false }
      - text/plain: Ø§Ù„Ø¨ÙˆØ¯ÙŠ ÙƒÙ„Ù‡ ÙŠÙƒÙˆÙ† JD ÙƒÙ†Øµ Ø¹Ø§Ø¯ÙŠ
      - multipart/form-data: job_description, debug
    """
    ctype = (request.headers.get("content-type") or "").lower()
    jd_text = ""
    debug = False

    if "application/json" in ctype:
        body = await request.json()
        jd_text = (body.get("job_description") or "").strip()
        debug = bool(body.get("debug", False))
    elif "text/plain" in ctype:
        raw = await request.body()
        jd_text = raw.decode("utf-8", errors="ignore").strip()
        debug = bool(request.query_params.get("debug", "false").lower() == "true")
    elif "multipart/form-data" in ctype:
        form = await request.form()
        jd_text = (form.get("job_description") or "").strip()
        debug = str(form.get("debug", "false")).lower() == "true"
    else:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© Ù†Øµ Ø®Ø§Ù…
        raw = await request.body()
        jd_text = raw.decode("utf-8", errors="ignore").strip()

    if not jd_text:
        return JSONResponse({"detail": "job description is empty"}, status_code=400)

    return infer_pipeline(jd_text, debug)

# (B) Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ù†Øµ Ø®Ø§Ù… ØµÙØ±Ù (Ù„Ù„ØªØ¨Ø³ÙŠØ· Ù„Ùˆ Ø­Ø§Ø¨Ø¨)
@app.post("/ai-job-title-text", response_model=JDResponse)
async def ai_job_title_text(body: str = Body(..., media_type="text/plain"), debug: bool=False):
    return infer_pipeline(body, debug)

# (C) Ø§Ø®ØªÙŠØ§Ø±ÙŠ: ÙÙˆØ±Ù…
@app.post("/ai-job-title-form", response_model=JDResponse)
async def ai_job_title_form(job_description: str = Form(...), debug: bool = Form(False)):
    return infer_pipeline(job_description, bool(debug))
